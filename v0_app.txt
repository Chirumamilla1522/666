# # # # # # import re
# # # # # # import logging
# # # # # # from fastapi import HTTPException, Query, Depends
# # # # # # import os
# # # # # # import ssl
# # # # # # import uuid
# # # # # # import datetime
# # # # # # import glob
# # # # # # import json
# # # # # # import logging
# # # # # # from pathlib import Path
# # # # # # from typing import List
# # # # # # import crawl
# # # # # # import warnings
# # # # # # from bs4 import GuessedAtParserWarning
# # # # # # warnings.filterwarnings("ignore", category=GuessedAtParserWarning)

# # # # # # import google.generativeai as genai
# # # # # # import wikipedia
# # # # # # import yfinance as yf
# # # # # # import pandas as pd
# # # # # # from fastapi import FastAPI, HTTPException, Depends, Query, BackgroundTasks
# # # # # # from fastapi.middleware.cors import CORSMiddleware
# # # # # # from fastapi.responses import FileResponse
# # # # # # from pydantic import BaseModel
# # # # # # from sqlalchemy import create_engine, Column, String, Float
# # # # # # from sqlalchemy.ext.declarative import declarative_base
# # # # # # from sqlalchemy.orm import sessionmaker, Session
# # # # # # from sentence_transformers import SentenceTransformer
# # # # # # import faiss
# # # # # # import numpy as np
# # # # # # from textblob import TextBlob

# # # # # # # New imports for TF-IDF
# # # # # # from sklearn.feature_extraction.text import TfidfVectorizer
# # # # # # from sklearn.metrics.pairwise import cosine_similarity

# # # # # # # ─── SSL & Logging ─────────────────────────────────────────────────────────────
# # # # # # ssl._create_default_https_context = ssl._create_unverified_context
# # # # # # logging.getLogger("yfinance").setLevel(logging.ERROR)

# # # # # # # ─── Load environment & configure Gemini ────────────────────────────────────────
# # # # # # from dotenv import load_dotenv
# # # # # # load_dotenv()
# # # # # # genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# # # # # # # ─── FastAPI setup ─────────────────────────────────────────────────────────────
# # # # # # app = FastAPI(title="StockRadar Core API")
# # # # # # app.add_middleware(
# # # # # #     CORSMiddleware,
# # # # # #     allow_origins=["*"],
# # # # # #     allow_methods=["*"],
# # # # # #     allow_headers=["*"],
# # # # # # )
# # # # # # @app.get("/", include_in_schema=False)
# # # # # # def serve_ui():
# # # # # #     return FileResponse("index.html")


# # # # # # # ─── Database (SQLite + SQLAlchemy) ─────────────────────────────────────────────
# # # # # # DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./stockradar.db")
# # # # # # engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
# # # # # # SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
# # # # # # Base = declarative_base()

# # # # # # class HoldingModel(Base):
# # # # # #     __tablename__ = "portfolio"
# # # # # #     id        = Column(String(36), primary_key=True, index=True)
# # # # # #     ticker    = Column(String(10), nullable=False, unique=True, index=True)
# # # # # #     quantity  = Column(Float, nullable=False)
# # # # # #     avg_price = Column(Float, nullable=False)

# # # # # # Base.metadata.create_all(bind=engine)
# # # # # # def get_db():
# # # # # #     db = SessionLocal()
# # # # # #     try:
# # # # # #         yield db
# # # # # #     finally:
# # # # # #         db.close()


# # # # # # # ─── Pydantic Schemas ──────────────────────────────────────────────────────────
# # # # # # class Holding(BaseModel):
# # # # # #     id: str
# # # # # #     ticker: str
# # # # # #     quantity: float
# # # # # #     avg_price: float

# # # # # # class HoldingCreate(BaseModel):
# # # # # #     ticker: str
# # # # # #     quantity: float

# # # # # # class Recommendation(BaseModel):
# # # # # #     ticker: str
# # # # # #     name: str
# # # # # #     sector: str
# # # # # #     similarity_score: float

# # # # # # class Quote(BaseModel):
# # # # # #     ticker: str
# # # # # #     name: str
# # # # # #     price: float
# # # # # #     change: float
# # # # # #     spark: List[float]

# # # # # # class Performer(BaseModel):
# # # # # #     ticker: str
# # # # # #     name: str
# # # # # #     sector: str
# # # # # #     change_percent: float

# # # # # # class NewsImpactAdvancedItem(BaseModel):
# # # # # #     ticker: str
# # # # # #     headline: str
# # # # # #     link: str
# # # # # #     source: str
# # # # # #     sentiment: float
# # # # # #     delta: float
# # # # # #     impact: float
# # # # # #     spark: List[float]
# # # # # #     summary: str
# # # # # #     published: datetime.datetime

# # # # # # class StockItem(BaseModel):
# # # # # #     ticker: str
# # # # # #     name: str


# # # # # # # ─── Load S&P 500 metadata ─────────────────────────────────────────────────────
# # # # # # wiki_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
# # # # # # df = pd.read_html(wiki_url, flavor="lxml")[0]
# # # # # # stock_keys = df["Symbol"].tolist()
# # # # # # stock_meta = {
# # # # # #     row.Symbol: {"name": row.Security, "sector": row["GICS Sector"]}
# # # # # #     for _, row in df.iterrows()
# # # # # # }


# # # # # # # ─── Build FAISS index for embeddings-based recommendations ────────────────────
# # # # # # model = SentenceTransformer("all-MiniLM-L6-v2")
# # # # # # texts = [f"{stock_meta[t]['name']} in {stock_meta[t]['sector']}" for t in stock_keys]
# # # # # # embs = model.encode(texts, convert_to_numpy=True, show_progress_bar=False).astype("float32")
# # # # # # faiss.normalize_L2(embs)
# # # # # # faiss_index = faiss.IndexFlatIP(embs.shape[1])
# # # # # # faiss_index.add(embs)


# # # # # # # ─── TF-IDF Setup for IR-based recommendations ────────────────────────────────
# # # # # # tfidf_corpus    = [f"{stock_meta[t]['name']} {stock_meta[t]['sector']}" for t in stock_keys]
# # # # # # tfidf_vectorizer= TfidfVectorizer(stop_words="english")
# # # # # # tfidf_matrix    = tfidf_vectorizer.fit_transform(tfidf_corpus)  # shape=(N_stocks, N_terms)


# # # # # # # ─── Portfolio CRUD ────────────────────────────────────────────────────────────
# # # # # # @app.get("/portfolio", response_model=List[Holding])
# # # # # # def list_portfolio(db: Session = Depends(get_db)):
# # # # # #     rows = db.query(HoldingModel).all()
# # # # # #     return [Holding(**r.__dict__) for r in rows]

# # # # # # @app.post("/portfolio", response_model=Holding, status_code=201)
# # # # # # def add_holding(h: HoldingCreate, db: Session = Depends(get_db)):
# # # # # #     t = h.ticker.upper()
# # # # # #     if db.query(HoldingModel).filter_by(ticker=t).first():
# # # # # #         raise HTTPException(400, "Ticker already in portfolio")
# # # # # #     # fetch current close for avg_price
# # # # # #     yf_sym = t.replace(".", "-")
# # # # # #     try:
# # # # # #         hist = yf.Ticker(yf_sym).history(period="1d")
# # # # # #         avg_price = float(hist["Close"].iloc[-1])
# # # # # #     except:
# # # # # #         avg_price = 0.0
# # # # # #     rec = HoldingModel(id=uuid.uuid4().hex, ticker=t, quantity=h.quantity, avg_price=avg_price)
# # # # # #     db.add(rec); db.commit(); db.refresh(rec)
# # # # # #     return Holding(**rec.__dict__)

# # # # # # @app.put("/portfolio/{holding_id}", response_model=Holding)
# # # # # # def update_holding(holding_id: str, h: HoldingCreate, db: Session = Depends(get_db)):
# # # # # #     rec = db.query(HoldingModel).get(holding_id)
# # # # # #     if not rec:
# # # # # #         raise HTTPException(404, "Holding not found")
# # # # # #     t = h.ticker.upper()
# # # # # #     yf_sym = t.replace(".", "-")
# # # # # #     try:
# # # # # #         hist = yf.Ticker(yf_sym).history(period="1d")
# # # # # #         avg_price = float(hist["Close"].iloc[-1])
# # # # # #     except:
# # # # # #         avg_price = rec.avg_price
# # # # # #     rec.ticker, rec.quantity, rec.avg_price = t, h.quantity, avg_price
# # # # # #     db.commit(); db.refresh(rec)
# # # # # #     return Holding(**rec.__dict__)

# # # # # # @app.delete("/portfolio/{holding_id}", status_code=204)
# # # # # # def delete_holding(holding_id: str, db: Session = Depends(get_db)):
# # # # # #     db.query(HoldingModel).filter_by(id=holding_id).delete()
# # # # # #     db.commit()


# # # # # # # ─── FAISS embedding-based recommendations ─────────────────────────────────────
# # # # # # # @app.get("/recommendations", response_model=List[Recommendation])
# # # # # # # def recommend(db: Session = Depends(get_db)):
# # # # # # #     holdings = db.query(HoldingModel).all()
# # # # # # #     owned    = {h.ticker for h in holdings}
# # # # # # #     if not owned:
# # # # # # #         raise HTTPException(400, "Empty portfolio")
# # # # # # #     idxs = [stock_keys.index(t) for t in owned if t in stock_keys]
# # # # # # #     centroid = np.mean(embs[idxs], axis=0, keepdims=True)
# # # # # # #     faiss.normalize_L2(centroid)
# # # # # # #     D, I = faiss_index.search(centroid, len(stock_keys))
# # # # # # #     recs = []
# # # # # # #     for score, idx in zip(D[0], I[0]):
# # # # # # #         t = stock_keys[idx]
# # # # # # #         if t in owned: continue
# # # # # # #         m = stock_meta[t]
# # # # # # #         recs.append(Recommendation(
# # # # # # #             ticker=t, name=m["name"], sector=m["sector"],
# # # # # # #             similarity_score=round(float(score), 3)
# # # # # # #         ))
# # # # # # #         if len(recs) >= 5: break
# # # # # # #     return recs


# # # # # # # ─── TF-IDF + cosine similarity recommendations ───────────────────────────────
# # # # # # @app.get("/recommendations", response_model=List[Recommendation])
# # # # # # def recommend(db: Session = Depends(get_db)):
# # # # # #         # 1) load portfolio tickers
# # # # # #     holdings = [h.ticker for h in db.query(HoldingModel).all()]
# # # # # #     if not holdings:
# # # # # #         raise HTTPException(400, "Portfolio is empty")

# # # # # #     # 2) map to TF-IDF rows
# # # # # #     idxs = [stock_keys.index(t) for t in holdings if t in stock_keys]
# # # # # #     if not idxs:
# # # # # #         raise HTTPException(400, "No metadata for portfolio tickers")

# # # # # #     # 3) compute portfolio centroid and convert to ndarray
# # # # # #     centroid = tfidf_matrix[idxs].mean(axis=0)       # returns a numpy.matrix
# # # # # #     centroid = centroid.A if hasattr(centroid, "A") else np.asarray(centroid)

# # # # # #     # 4) cosine similarities
# # # # # #     sims = cosine_similarity(centroid, tfidf_matrix).flatten()

# # # # # #     # 5) top-5 non-held tickers
# # # # # #     recs = []
# # # # # #     for idx in sims.argsort()[::-1]:
# # # # # #         tk = stock_keys[idx]
# # # # # #         if tk in holdings:
# # # # # #             continue
# # # # # #         m = stock_meta[tk]
# # # # # #         recs.append(Recommendation(
# # # # # #             ticker=tk,
# # # # # #             name=m["name"],
# # # # # #             sector=m["sector"],
# # # # # #             similarity_score=round(float(sims[idx]), 3)
# # # # # #         ))
# # # # # #         if len(recs) >= 10:
# # # # # #             break

# # # # # #     return recs

# # # # # #     # holdings = [h.ticker for h in db.query(HoldingModel).all()]
# # # # # #     # if not holdings:
# # # # # #     #     raise HTTPException(400, "Portfolio is empty")
# # # # # #     # idxs = [stock_keys.index(t) for t in holdings if t in stock_keys]
# # # # # #     # if not idxs:
# # # # # #     #     raise HTTPException(400, "No metadata for portfolio tickers")
# # # # # #     # centroid = tfidf_matrix[idxs].mean(axis=0)  # shape=(1,terms)
# # # # # #     # sims     = cosine_similarity(centroid, tfidf_matrix).flatten()
# # # # # #     # recs = []
# # # # # #     # for idx in sims.argsort()[::-1]:
# # # # # #     #     tk = stock_keys[idx]
# # # # # #     #     if tk in holdings: continue
# # # # # #     #     m = stock_meta[tk]
# # # # # #     #     recs.append(Recommendation(
# # # # # #     #         ticker=tk, name=m["name"], sector=m["sector"],
# # # # # #     #         similarity_score=round(float(sims[idx]), 3)
# # # # # #     #     ))
# # # # # #     #     if len(recs) >= 5: break
# # # # # #     # return recs


# # # # # # # ─── Quotes Endpoint ────────────────────────────────────────────────────────────
# # # # # # @app.get("/quotes", response_model=List[Quote])
# # # # # # def get_quotes(db: Session = Depends(get_db)):
# # # # # #     holdings = db.query(HoldingModel).all()
# # # # # #     out = []
# # # # # #     for h in holdings:
# # # # # #         yf_sym = h.ticker.replace(".", "-")
# # # # # #         try:
# # # # # #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# # # # # #             if hist is None or hist.empty: continue
# # # # # #             opens, closes = hist["Open"].tolist(), hist["Close"].tolist()
# # # # # #             delta = round(closes[-1] - opens[0], 2)
# # # # # #             out.append(Quote(
# # # # # #                 ticker=h.ticker,
# # # # # #                 name=stock_meta.get(h.ticker,{}).get("name",h.ticker),
# # # # # #                 price=round(closes[-1], 2),
# # # # # #                 change=delta,
# # # # # #                 spark=closes
# # # # # #             ))
# # # # # #         except:
# # # # # #             continue
# # # # # #     if not out:
# # # # # #         raise HTTPException(404, "No quote data")
# # # # # #     return out


# # # # # # @app.get("/stocks", response_model=List[StockItem])
# # # # # # def list_stocks():
# # # # # #     return [{"ticker": tk, "name": stock_meta[tk]["name"]} for tk in stock_keys]


# # # # # # # ─── Top Performers Endpoint ───────────────────────────────────────────────────
# # # # # # @app.get("/top-performers", response_model=List[Performer])
# # # # # # def top_performers():
# # # # # #     perf = []
# # # # # #     for tk in stock_keys:
# # # # # #         yf_sym = tk.replace(".", "-")
# # # # # #         try:
# # # # # #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# # # # # #             if hist is None or hist.empty: continue
# # # # # #             opens, closes = hist["Open"].tolist(), hist["Close"].tolist()
# # # # # #             pct = round((closes[-1] - opens[0]) / opens[0] * 100, 2)
# # # # # #             perf.append({
# # # # # #                 "ticker": tk,
# # # # # #                 "name": stock_meta[tk]["name"],
# # # # # #                 "sector": stock_meta[tk]["sector"],
# # # # # #                 "change_percent": pct
# # # # # #             })
# # # # # #         except:
# # # # # #             continue
# # # # # #     if not perf:
# # # # # #         raise HTTPException(404, "No performance data")
# # # # # #     return sorted(perf, key=lambda x: x["change_percent"], reverse=True)[:5]

# # # # # # @app.get("/news-impact", response_model=List[NewsImpactAdvancedItem])
# # # # # # def news_impact(
# # # # # #     background_tasks: BackgroundTasks,
# # # # # #     db: Session            = Depends(get_db),
# # # # # #     window: int            = Query(60, ge=1),
# # # # # #     min_delta: float       = Query(0.0, ge=0.0),
# # # # # #     min_sentiment: float   = Query(0.0, ge=0.0),
# # # # # #     recent_hours: int      = Query(24, ge=1),
# # # # # #     limit: int             = Query(20, ge=1, le=100),
# # # # # # ):
# # # # # #     # kick off a background crawl
# # # # # #     background_tasks.add_task(crawl.main)

# # # # # #     try:
# # # # # #         now    = datetime.datetime.utcnow()
# # # # # #         cutoff = now - datetime.timedelta(hours=recent_hours)

# # # # # #         holdings = db.query(HoldingModel).all()
# # # # # #         recs     = recommend(db)
# # # # # #         tickers  = {h.ticker for h in holdings} | {r.ticker for r in recs}

# # # # # #         # build regex to match any ticker as a whole word
# # # # # #         ticker_pattern = re.compile(r"\b(" + "|".join(map(re.escape, tickers)) + r")\b")

# # # # # #         raw = []
# # # # # #         for fn in glob.glob("data/raw_articles/rss_articles_*.json"):
# # # # # #             raw.extend(json.loads(Path(fn).read_text()))

# # # # # #         scored = []
# # # # # #         for art in raw:
# # # # # #             pub = art.get("published","")
# # # # # #             if not pub:
# # # # # #                 continue

# # # # # #             # parse ISO8601, handling trailing 'Z' → '+00:00'
# # # # # #             try:
# # # # # #                 if pub.endswith("Z"):
# # # # # #                     pub_dt = datetime.datetime.fromisoformat(pub[:-1] + "+00:00")
# # # # # #                 else:
# # # # # #                     pub_dt = datetime.datetime.fromisoformat(pub)
# # # # # #                 # drop tzinfo so it matches naive UTC now/cutoff
# # # # # #                 pub_dt = pub_dt.replace(tzinfo=None)
# # # # # #             except Exception:
# # # # # #                 continue

# # # # # #             # filter by time window
# # # # # #             if pub_dt < cutoff:
# # # # # #                 continue

# # # # # #             text = f"{art.get('title','')} {art.get('summary','')}".strip()
# # # # # #             m = ticker_pattern.search(text)
# # # # # #             if not m:
# # # # # #                 continue
# # # # # #             tick = m.group(1).upper()

# # # # # #             # sentiment filtering
# # # # # #             pol = TextBlob(text).sentiment.polarity
# # # # # #             if abs(pol) < min_sentiment:
# # # # # #                 continue

# # # # # #             # fetch 1m history around publication
# # # # # #             yf_sym = tick.replace(".", "-")
# # # # # #             df     = yf.Ticker(yf_sym).history(
# # # # # #                 start=pub_dt - datetime.timedelta(minutes=30),
# # # # # #                 end=pub_dt + datetime.timedelta(minutes=window+30),
# # # # # #                 interval="1m"
# # # # # #             )
# # # # # #             if df is None or df.empty:
# # # # # #                 continue
# # # # # #             closes = df["Close"].tolist()
# # # # # #             before, after = closes[0], closes[min(window, len(closes)-1)]
# # # # # #             delta = round(after - before, 2)
# # # # # #             if abs(delta) < min_delta:
# # # # # #                 continue

# # # # # #             impact = round(abs(delta) * abs(pol), 4)

# # # # # #             # optional Gemini summary
# # # # # #             summary = ""
# # # # # #             try:
# # # # # #                 gem = genai.generate_text(
# # # # # #                     model="models/chat-bison-001",
# # # # # #                     prompt=(
# # # # # #                       f"Headline: {art['title']}\n\n"
# # # # # #                       f"Why might this have moved {tick} stock?"
# # # # # #                     ),
# # # # # #                     temperature=0.7,
# # # # # #                     max_output_tokens=50,
# # # # # #                 )
# # # # # #                 summary = gem.text.strip()
# # # # # #             except Exception:
# # # # # #                 logging.warning("Gemini summary failed for %s", art.get("link",""))

# # # # # #             scored.append({
# # # # # #                 "ticker":    tick,
# # # # # #                 "headline":  art["title"],
# # # # # #                 "link":      art.get("link",""),
# # # # # #                 "source":    art.get("source","Unknown"),
# # # # # #                 "sentiment": round(pol,3),
# # # # # #                 "delta":     delta,
# # # # # #                 "impact":    impact,
# # # # # #                 "spark":     closes,
# # # # # #                 "summary":   summary,
# # # # # #                 "published": pub_dt
# # # # # #             })

# # # # # #         top = sorted(scored, key=lambda x: x["impact"], reverse=True)[:limit]
# # # # # #         return top

# # # # # #     except Exception:
# # # # # #         logging.exception("Error in /news-impact")
# # # # # #         raise HTTPException(500, "News impact processing failed")

# # # # # import os
# # # # # import ssl
# # # # # import uuid
# # # # # import datetime
# # # # # import glob
# # # # # import json
# # # # # import logging
# # # # # import warnings
# # # # # from pathlib import Path
# # # # # from typing import List

# # # # # # ─── Suppress warnings ─────────────────────────────────────────────────────────
# # # # # from bs4 import GuessedAtParserWarning
# # # # # from requests.packages.urllib3.exceptions import InsecureRequestWarning

# # # # # warnings.filterwarnings("ignore", category=GuessedAtParserWarning)
# # # # # warnings.filterwarnings("ignore", category=InsecureRequestWarning)

# # # # # # ─── Logging & SSL ─────────────────────────────────────────────────────────────
# # # # # ssl._create_default_https_context = ssl._create_unverified_context

# # # # # # Silence yfinance “possibly delisted” noise
# # # # # logging.getLogger("yfinance").setLevel(logging.CRITICAL)
# # # # # logging.getLogger("yfinance.ticker").setLevel(logging.CRITICAL)

# # # # # # ─── Third-party imports ────────────────────────────────────────────────────────
# # # # # import google.generativeai as genai
# # # # # import wikipedia
# # # # # import yfinance as yf
# # # # # import pandas as pd
# # # # # from fastapi import (
# # # # #     FastAPI, HTTPException, Depends, Query,
# # # # #     BackgroundTasks
# # # # # )
# # # # # from fastapi.middleware.cors import CORSMiddleware
# # # # # from fastapi.responses import FileResponse
# # # # # from pydantic import BaseModel
# # # # # from sqlalchemy import create_engine, Column, String, Float
# # # # # from sqlalchemy.ext.declarative import declarative_base
# # # # # from sqlalchemy.orm import sessionmaker, Session
# # # # # from sklearn.feature_extraction.text import TfidfVectorizer
# # # # # from sklearn.metrics.pairwise import cosine_similarity
# # # # # from sentence_transformers import SentenceTransformer
# # # # # import faiss
# # # # # import numpy as np
# # # # # from textblob import TextBlob
# # # # # import crawl  # your crawl_rss.py exposing main()

# # # # # # ─── Load env & configure Gemini ────────────────────────────────────────────────
# # # # # from dotenv import load_dotenv
# # # # # load_dotenv()
# # # # # genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# # # # # # ─── FastAPI setup ─────────────────────────────────────────────────────────────
# # # # # app = FastAPI(title="StockRadar Core API")
# # # # # app.add_middleware(
# # # # #     CORSMiddleware,
# # # # #     allow_origins=["*"],
# # # # #     allow_methods=["*"],
# # # # #     allow_headers=["*"],
# # # # # )
# # # # # @app.get("/", include_in_schema=False)
# # # # # def serve_ui():
# # # # #     return FileResponse("index.html")


# # # # # # ─── Database (SQLite + SQLAlchemy) ─────────────────────────────────────────────
# # # # # DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./stockradar.db")
# # # # # engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
# # # # # SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
# # # # # Base = declarative_base()

# # # # # class HoldingModel(Base):
# # # # #     __tablename__ = "portfolio"
# # # # #     id        = Column(String(36), primary_key=True, index=True)
# # # # #     ticker    = Column(String(10), nullable=False, unique=True, index=True)
# # # # #     quantity  = Column(Float, nullable=False)
# # # # #     avg_price = Column(Float, nullable=False)

# # # # # Base.metadata.create_all(bind=engine)

# # # # # def get_db():
# # # # #     db = SessionLocal()
# # # # #     try:
# # # # #         yield db
# # # # #     finally:
# # # # #         db.close()


# # # # # # ─── Pydantic Schemas ──────────────────────────────────────────────────────────
# # # # # class Holding(BaseModel):
# # # # #     id: str
# # # # #     ticker: str
# # # # #     quantity: float
# # # # #     avg_price: float

# # # # # class HoldingCreate(BaseModel):
# # # # #     ticker: str
# # # # #     quantity: float

# # # # # class Recommendation(BaseModel):
# # # # #     ticker: str
# # # # #     name: str
# # # # #     sector: str
# # # # #     similarity_score: float

# # # # # class Quote(BaseModel):
# # # # #     ticker: str
# # # # #     name: str
# # # # #     price: float
# # # # #     change: float
# # # # #     spark: List[float]

# # # # # class Performer(BaseModel):
# # # # #     ticker: str
# # # # #     name: str
# # # # #     sector: str
# # # # #     change_percent: float

# # # # # class NewsImpactItem(BaseModel):
# # # # #     ticker: str
# # # # #     headline: str
# # # # #     link: str
# # # # #     source: str
# # # # #     sentiment: float
# # # # #     delta: float
# # # # #     impact: float
# # # # #     spark: List[float]
# # # # #     summary: str
# # # # #     published: datetime.datetime

# # # # # class StockItem(BaseModel):
# # # # #     ticker: str
# # # # #     name: str


# # # # # # ─── Load S&P 500 metadata ─────────────────────────────────────────────────────
# # # # # wiki_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
# # # # # df = pd.read_html(wiki_url, flavor="lxml")[0]
# # # # # stock_keys = df["Symbol"].tolist()
# # # # # stock_meta = {
# # # # #     row.Symbol: {"name": row.Security, "sector": row["GICS Sector"]}
# # # # #     for _, row in df.iterrows()
# # # # # }


# # # # # # ─── TF-IDF Setup for IR‐based recommendations ────────────────────────────────
# # # # # tfidf_corpus     = [f"{stock_meta[t]['name']} {stock_meta[t]['sector']}" for t in stock_keys]
# # # # # tfidf_vectorizer = TfidfVectorizer(stop_words="english")
# # # # # tfidf_matrix     = tfidf_vectorizer.fit_transform(tfidf_corpus)  # (N_stocks x N_terms)


# # # # # # ─── Portfolio CRUD ────────────────────────────────────────────────────────────
# # # # # @app.get("/portfolio", response_model=List[Holding])
# # # # # def list_portfolio(db: Session = Depends(get_db)):
# # # # #     rows = db.query(HoldingModel).all()
# # # # #     return [Holding(**r.__dict__) for r in rows]

# # # # # @app.post("/portfolio", response_model=Holding, status_code=201)
# # # # # def add_holding(h: HoldingCreate, db: Session = Depends(get_db)):
# # # # #     t = h.ticker.upper()
# # # # #     if db.query(HoldingModel).filter_by(ticker=t).first():
# # # # #         raise HTTPException(400, "Ticker already in portfolio")
# # # # #     yf_sym = t.replace(".", "-")
# # # # #     try:
# # # # #         hist = yf.Ticker(yf_sym).history(period="1d")
# # # # #         avg_price = float(hist["Close"].iloc[-1])
# # # # #     except:
# # # # #         avg_price = 0.0
# # # # #     rec = HoldingModel(
# # # # #         id=uuid.uuid4().hex,
# # # # #         ticker=t,
# # # # #         quantity=h.quantity,
# # # # #         avg_price=avg_price
# # # # #     )
# # # # #     db.add(rec); db.commit(); db.refresh(rec)
# # # # #     return Holding(**rec.__dict__)

# # # # # @app.put("/portfolio/{holding_id}", response_model=Holding)
# # # # # def update_holding(holding_id: str, h: HoldingCreate, db: Session = Depends(get_db)):
# # # # #     rec = db.query(HoldingModel).get(holding_id)
# # # # #     if not rec:
# # # # #         raise HTTPException(404, "Holding not found")
# # # # #     t = h.ticker.upper()
# # # # #     yf_sym = t.replace(".", "-")
# # # # #     try:
# # # # #         hist = yf.Ticker(yf_sym).history(period="1d")
# # # # #         avg_price = float(hist["Close"].iloc[-1])
# # # # #     except:
# # # # #         avg_price = rec.avg_price
# # # # #     rec.ticker, rec.quantity, rec.avg_price = t, h.quantity, avg_price
# # # # #     db.commit(); db.refresh(rec)
# # # # #     return Holding(**rec.__dict__)

# # # # # @app.delete("/portfolio/{holding_id}", status_code=204)
# # # # # def delete_holding(holding_id: str, db: Session = Depends(get_db)):
# # # # #     db.query(HoldingModel).filter_by(id=holding_id).delete()
# # # # #     db.commit()


# # # # # # ─── Stocks list for Autocomplete ──────────────────────────────────────────────
# # # # # @app.get("/stocks", response_model=List[StockItem])
# # # # # def list_stocks():
# # # # #     return [{"ticker": tk, "name": stock_meta[tk]["name"]} for tk in stock_keys]


# # # # # # ─── Quotes Endpoint ────────────────────────────────────────────────────────────
# # # # # @app.get("/quotes", response_model=List[Quote])
# # # # # def get_quotes(db: Session = Depends(get_db)):
# # # # #     holdings = db.query(HoldingModel).all()
# # # # #     out = []
# # # # #     for h in holdings:
# # # # #         yf_sym = h.ticker.replace(".", "-")
# # # # #         try:
# # # # #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# # # # #             if hist is None or hist.empty:
# # # # #                 continue
# # # # #             opens, closes = hist["Open"].tolist(), hist["Close"].tolist()
# # # # #             delta = round(closes[-1] - opens[0], 2)
# # # # #             out.append(Quote(
# # # # #                 ticker=h.ticker,
# # # # #                 name=stock_meta.get(h.ticker,{}).get("name",h.ticker),
# # # # #                 price=round(closes[-1], 2),
# # # # #                 change=delta,
# # # # #                 spark=closes
# # # # #             ))
# # # # #         except:
# # # # #             continue
# # # # #     if not out:
# # # # #         raise HTTPException(404, "No quote data")
# # # # #     return out


# # # # # # ─── Top Performers Endpoint ───────────────────────────────────────────────────
# # # # # @app.get("/top-performers", response_model=List[Performer])
# # # # # def top_performers():
# # # # #     perf = []
# # # # #     for tk in stock_keys:
# # # # #         yf_sym = tk.replace(".", "-")
# # # # #         try:
# # # # #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# # # # #             if hist is None or hist.empty:
# # # # #                 continue
# # # # #             opens, closes = hist["Open"].tolist(), hist["Close"].tolist()
# # # # #             pct = round((closes[-1] - opens[0]) / opens[0] * 100, 2)
# # # # #             perf.append({
# # # # #                 "ticker": tk,
# # # # #                 "name": stock_meta[tk]["name"],
# # # # #                 "sector": stock_meta[tk]["sector"],
# # # # #                 "change_percent": pct
# # # # #             })
# # # # #         except:
# # # # #             continue
# # # # #     if not perf:
# # # # #         raise HTTPException(404, "No performance data")
# # # # #     return sorted(perf, key=lambda x: x["change_percent"], reverse=True)[:5]


# # # # # # ─── TF-IDF + cosine similarity recommendations ───────────────────────────────
# # # # # @app.get("/recommendations", response_model=List[Recommendation])
# # # # # def recommend(db: Session = Depends(get_db)):
# # # # #     holdings = [h.ticker for h in db.query(HoldingModel).all()]
# # # # #     if not holdings:
# # # # #         raise HTTPException(400, "Portfolio is empty")
# # # # #     idxs = [stock_keys.index(t) for t in holdings if t in stock_keys]
# # # # #     if not idxs:
# # # # #         raise HTTPException(400, "No metadata for portfolio tickers")

# # # # #     centroid = tfidf_matrix[idxs].mean(axis=0)
# # # # #     centroid = centroid.A if hasattr(centroid, "A") else np.asarray(centroid)
# # # # #     sims = cosine_similarity(centroid, tfidf_matrix).flatten()

# # # # #     recs = []
# # # # #     for idx in sims.argsort()[::-1]:
# # # # #         tk = stock_keys[idx]
# # # # #         if tk in holdings:
# # # # #             continue
# # # # #         m = stock_meta[tk]
# # # # #         recs.append(Recommendation(
# # # # #             ticker=tk,
# # # # #             name=m["name"],
# # # # #             sector=m["sector"],
# # # # #             similarity_score=round(float(sims[idx]), 3)
# # # # #         ))
# # # # #         if len(recs) >= 5:
# # # # #             break
# # # # #     return recs


# # # # # # ─── News Impact Endpoint (auto-run crawler + robust parsing) ─────────────────
# # # # # import re

# # # # # @app.get("/news-impact", response_model=List[NewsImpactItem])
# # # # # def news_impact(
# # # # #     background_tasks: BackgroundTasks,
# # # # #     db: Session            = Depends(get_db),
# # # # #     window: int            = Query(60, ge=1),
# # # # #     min_delta: float       = Query(0.0, ge=0.0),
# # # # #     min_sentiment: float   = Query(0.0, ge=0.0),
# # # # #     recent_hours: int      = Query(24, ge=1),
# # # # #     limit: int             = Query(20, ge=1, le=100),
# # # # # ):
# # # # #     # schedule an async crawl
# # # # #     background_tasks.add_task(crawl.main)

# # # # #     try:
# # # # #         now    = datetime.datetime.utcnow()
# # # # #         cutoff = now - datetime.timedelta(hours=recent_hours)

# # # # #         holdings = db.query(HoldingModel).all()
# # # # #         recs     = recommend(db)
# # # # #         tickers  = {h.ticker for h in holdings} | {r.ticker for r in recs}

# # # # #         # regex for any ticker as a whole word
# # # # #         ticker_rx = re.compile(r"\b(" + "|".join(map(re.escape, tickers)) + r")\b")

# # # # #         raw = []
# # # # #         for fn in glob.glob("data/raw_articles/rss_articles_*.json"):
# # # # #             raw.extend(json.loads(Path(fn).read_text()))

# # # # #         scored = []
# # # # #         for art in raw:
# # # # #             pub = art.get("published","")
# # # # #             if not pub:
# # # # #                 continue

# # # # #             # parse ISO8601 with Z or offset, then drop tzinfo
# # # # #             try:
# # # # #                 if pub.endswith("Z"):
# # # # #                     dt = datetime.datetime.fromisoformat(pub[:-1] + "+00:00")
# # # # #                 else:
# # # # #                     dt = datetime.datetime.fromisoformat(pub)
# # # # #                 pub_dt = dt.replace(tzinfo=None)
# # # # #             except:
# # # # #                 continue

# # # # #             if pub_dt < cutoff:
# # # # #                 continue

# # # # #             text = f"{art.get('title','')} {art.get('summary','')}".strip()
# # # # #             m = ticker_rx.search(text)
# # # # #             if not m:
# # # # #                 continue
# # # # #             tick = m.group(1).upper()

# # # # #             pol = TextBlob(text).sentiment.polarity
# # # # #             if abs(pol) < min_sentiment:
# # # # #                 continue

# # # # #             # fetch around the event
# # # # #             try:
# # # # #                 df = yf.Ticker(tick.replace(".", "-")).history(
# # # # #                     start=pub_dt - datetime.timedelta(minutes=30),
# # # # #                     end=  pub_dt + datetime.timedelta(minutes=window+30),
# # # # #                     interval="1m"
# # # # #                 )
# # # # #                 if df is None or df.empty:
# # # # #                     continue
# # # # #                 closes = df["Close"].tolist()
# # # # #             except:
# # # # #                 continue

# # # # #             before, after = closes[0], closes[min(window, len(closes)-1)]
# # # # #             delta = round(after - before, 2)
# # # # #             if abs(delta) < min_delta:
# # # # #                 continue

# # # # #             impact = round(abs(delta) * abs(pol), 4)

# # # # #             # optional Gemini summary
# # # # #             summary = ""
# # # # #             try:
# # # # #                 gem = genai.generate_text(
# # # # #                     model="models/chat-bison-001",
# # # # #                     prompt=(
# # # # #                       f"Headline: {art['title']}\n\n"
# # # # #                       f"Why might this have moved {tick} stock?"
# # # # #                     ),
# # # # #                     temperature=0.7,
# # # # #                     max_output_tokens=50,
# # # # #                 )
# # # # #                 summary = gem.text.strip()
# # # # #             except:
# # # # #                 logging.warning("Gemini summary failed for %s", art.get("link",""))

# # # # #             scored.append({
# # # # #                 "ticker":    tick,
# # # # #                 "headline":  art["title"],
# # # # #                 "link":      art.get("link",""),
# # # # #                 "source":    art.get("source","Unknown"),
# # # # #                 "sentiment": round(pol,3),
# # # # #                 "delta":     delta,
# # # # #                 "impact":    impact,
# # # # #                 "spark":     closes,
# # # # #                 "summary":   summary,
# # # # #                 "published": pub_dt
# # # # #             })

# # # # #         top = sorted(scored, key=lambda x: x["impact"], reverse=True)[:limit]
# # # # #         return top

# # # # #     except Exception:
# # # # #         logging.exception("Error in /news-impact")
# # # # #         raise HTTPException(500, "News impact processing failed")

# # # # import os
# # # # import ssl
# # # # import uuid
# # # # import datetime
# # # # import glob
# # # # import json
# # # # import logging
# # # # import warnings
# # # # from pathlib import Path
# # # # from typing import List

# # # # # ─── Suppress warnings ─────────────────────────────────────────────────────────
# # # # from bs4 import GuessedAtParserWarning
# # # # from requests.packages.urllib3.exceptions import InsecureRequestWarning

# # # # warnings.filterwarnings("ignore", category=GuessedAtParserWarning)
# # # # warnings.filterwarnings("ignore", category=InsecureRequestWarning)

# # # # # ─── Logging & SSL ─────────────────────────────────────────────────────────────
# # # # ssl._create_default_https_context = ssl._create_unverified_context
# # # # logging.getLogger("yfinance").setLevel(logging.CRITICAL)
# # # # logging.getLogger("yfinance.ticker").setLevel(logging.CRITICAL)

# # # # # ─── Third-party imports ────────────────────────────────────────────────────────
# # # # import yfinance as yf
# # # # import pandas as pd
# # # # from fastapi import (
# # # #     FastAPI, HTTPException, Depends, Query, BackgroundTasks
# # # # )
# # # # from fastapi.middleware.cors import CORSMiddleware
# # # # from fastapi.responses import FileResponse
# # # # from pydantic import BaseModel
# # # # from sqlalchemy import create_engine, Column, String, Float
# # # # from sqlalchemy.ext.declarative import declarative_base
# # # # from sqlalchemy.orm import sessionmaker, Session
# # # # from sklearn.feature_extraction.text import TfidfVectorizer
# # # # from sklearn.metrics.pairwise import cosine_similarity
# # # # from textblob import TextBlob
# # # # import re

# # # # import crawl  # your crawler script, exposes main()

# # # # from dotenv import load_dotenv
# # # # load_dotenv()
# # # # import google.generativeai as genai
# # # # genai.configure(api_key=os.getenv("GEMINI_API_KEY"))


# # # # # ─── FastAPI setup ─────────────────────────────────────────────────────────────
# # # # app = FastAPI(title="StockRadar Core API")
# # # # app.add_middleware(
# # # #     CORSMiddleware,
# # # #     allow_origins=["*"],
# # # #     allow_methods=["*"],
# # # #     allow_headers=["*"],
# # # # )

# # # # @app.get("/", include_in_schema=False)
# # # # def serve_ui():
# # # #     return FileResponse("index.html")


# # # # # ─── Database (SQLite + SQLAlchemy) ─────────────────────────────────────────────
# # # # DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./stockradar.db")
# # # # engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
# # # # SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
# # # # Base = declarative_base()

# # # # class HoldingModel(Base):
# # # #     __tablename__ = "portfolio"
# # # #     id        = Column(String(36), primary_key=True, index=True)
# # # #     ticker    = Column(String(10), nullable=False, unique=True, index=True)
# # # #     quantity  = Column(Float, nullable=False)
# # # #     avg_price = Column(Float, nullable=False)

# # # # Base.metadata.create_all(bind=engine)

# # # # def get_db():
# # # #     db = SessionLocal()
# # # #     try:
# # # #         yield db
# # # #     finally:
# # # #         db.close()


# # # # # ─── Pydantic Schemas ──────────────────────────────────────────────────────────
# # # # class Holding(BaseModel):
# # # #     id: str
# # # #     ticker: str
# # # #     quantity: float
# # # #     avg_price: float

# # # # class HoldingCreate(BaseModel):
# # # #     ticker: str
# # # #     quantity: float

# # # # class Recommendation(BaseModel):
# # # #     ticker: str
# # # #     name: str
# # # #     sector: str
# # # #     similarity_score: float

# # # # class Quote(BaseModel):
# # # #     ticker: str
# # # #     name: str
# # # #     price: float
# # # #     change: float
# # # #     spark: List[float]

# # # # class Performer(BaseModel):
# # # #     ticker: str
# # # #     name: str
# # # #     sector: str
# # # #     change_percent: float

# # # # class NewsImpactItem(BaseModel):
# # # #     ticker: str
# # # #     headline: str
# # # #     link: str
# # # #     source: str
# # # #     sentiment: float
# # # #     impact: float
# # # #     spark: List[float]
# # # #     summary: str
# # # #     published: datetime.datetime

# # # # class StockItem(BaseModel):
# # # #     ticker: str
# # # #     name: str


# # # # # ─── Load S&P 500 metadata ─────────────────────────────────────────────────────
# # # # wiki_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
# # # # df = pd.read_html(wiki_url, flavor="lxml")[0]
# # # # stock_keys = df["Symbol"].tolist()
# # # # stock_meta = {
# # # #     row.Symbol: {"name": row.Security, "sector": row["GICS Sector"]}
# # # #     for _, row in df.iterrows()
# # # # }


# # # # # ─── TF-IDF Setup for IR-based recommendations ────────────────────────────────
# # # # tfidf_corpus     = [f"{stock_meta[t]['name']} {stock_meta[t]['sector']}" for t in stock_keys]
# # # # tfidf_vectorizer = TfidfVectorizer(stop_words="english")
# # # # tfidf_matrix     = tfidf_vectorizer.fit_transform(tfidf_corpus)


# # # # # ─── Portfolio Endpoints ───────────────────────────────────────────────────────
# # # # @app.get("/portfolio", response_model=List[Holding])
# # # # def list_portfolio(db: Session = Depends(get_db)):
# # # #     rows = db.query(HoldingModel).all()
# # # #     return [Holding(**r.__dict__) for r in rows]

# # # # @app.post("/portfolio", response_model=Holding, status_code=201)
# # # # def add_holding(h: HoldingCreate, db: Session = Depends(get_db)):
# # # #     t = h.ticker.upper()
# # # #     if db.query(HoldingModel).filter_by(ticker=t).first():
# # # #         raise HTTPException(400, "Ticker already in portfolio")
# # # #     yf_sym = t.replace(".", "-")
# # # #     try:
# # # #         hist = yf.Ticker(yf_sym).history(period="1d")
# # # #         avg_price = float(hist["Close"].iloc[-1])
# # # #     except:
# # # #         avg_price = 0.0
# # # #     rec = HoldingModel(id=uuid.uuid4().hex, ticker=t,
# # # #                        quantity=h.quantity, avg_price=avg_price)
# # # #     db.add(rec); db.commit(); db.refresh(rec)
# # # #     return Holding(**rec.__dict__)

# # # # @app.put("/portfolio/{holding_id}", response_model=Holding)
# # # # def update_holding(holding_id: str, h: HoldingCreate, db: Session = Depends(get_db)):
# # # #     rec = db.query(HoldingModel).get(holding_id)
# # # #     if not rec:
# # # #         raise HTTPException(404, "Holding not found")
# # # #     t = h.ticker.upper()
# # # #     yf_sym = t.replace(".", "-")
# # # #     try:
# # # #         hist = yf.Ticker(yf_sym).history(period="1d")
# # # #         avg_price = float(hist["Close"].iloc[-1])
# # # #     except:
# # # #         avg_price = rec.avg_price
# # # #     rec.ticker, rec.quantity, rec.avg_price = t, h.quantity, avg_price
# # # #     db.commit(); db.refresh(rec)
# # # #     return Holding(**rec.__dict__)

# # # # @app.delete("/portfolio/{holding_id}", status_code=204)
# # # # def delete_holding(holding_id: str, db: Session = Depends(get_db)):
# # # #     db.query(HoldingModel).filter_by(id=holding_id).delete()
# # # #     db.commit()


# # # # # ─── Stocks list for Autocomplete ──────────────────────────────────────────────
# # # # @app.get("/stocks", response_model=List[StockItem])
# # # # def list_stocks():
# # # #     return [{"ticker": tk, "name": stock_meta[tk]["name"]} for tk in stock_keys]


# # # # # ─── Quotes Endpoint ────────────────────────────────────────────────────────────
# # # # @app.get("/quotes", response_model=List[Quote])
# # # # def get_quotes(db: Session = Depends(get_db)):
# # # #     holdings = db.query(HoldingModel).all()
# # # #     out = []
# # # #     for h in holdings:
# # # #         yf_sym = h.ticker.replace(".", "-")
# # # #         try:
# # # #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# # # #             if hist is None or hist.empty:
# # # #                 continue
# # # #             opens, closes = hist["Open"].tolist(), hist["Close"].tolist()
# # # #             delta = round(closes[-1] - opens[0], 2)
# # # #             out.append(Quote(
# # # #                 ticker=h.ticker,
# # # #                 name=stock_meta.get(h.ticker,{}).get("name",h.ticker),
# # # #                 price=round(closes[-1], 2),
# # # #                 change=delta,
# # # #                 spark=closes
# # # #             ))
# # # #         except:
# # # #             continue
# # # #     if not out:
# # # #         raise HTTPException(404, "No quote data")
# # # #     return out


# # # # # ─── Top Performers Endpoint ───────────────────────────────────────────────────
# # # # @app.get("/top-performers", response_model=List[Performer])
# # # # def top_performers():
# # # #     perf = []
# # # #     for tk in stock_keys:
# # # #         yf_sym = tk.replace(".", "-")
# # # #         try:
# # # #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# # # #             if hist is None or hist.empty:
# # # #                 continue
# # # #             opens, closes = hist["Open"].tolist(), hist["Close"].tolist()
# # # #             pct = round((closes[-1] - opens[0]) / opens[0] * 100, 2)
# # # #             perf.append({
# # # #                 "ticker": tk,
# # # #                 "name": stock_meta[tk]["name"],
# # # #                 "sector": stock_meta[tk]["sector"],
# # # #                 "change_percent": pct
# # # #             })
# # # #         except:
# # # #             continue
# # # #     if not perf:
# # # #         raise HTTPException(404, "No performance data")
# # # #     return sorted(perf, key=lambda x: x["change_percent"], reverse=True)[:5]


# # # # # ─── TF-IDF Recommendations ────────────────────────────────────────────────────
# # # # @app.get("/recommendations", response_model=List[Recommendation])
# # # # def recommend(db: Session = Depends(get_db)):
# # # #     holdings = [h.ticker for h in db.query(HoldingModel).all()]
# # # #     if not holdings:
# # # #         raise HTTPException(400, "Portfolio is empty")
# # # #     idxs = [stock_keys.index(t) for t in holdings if t in stock_keys]
# # # #     if not idxs:
# # # #         raise HTTPException(400, "No metadata for portfolio tickers")

# # # #     centroid = tfidf_matrix[idxs].mean(axis=0)
# # # #     centroid = centroid.A if hasattr(centroid, "A") else np.asarray(centroid)
# # # #     sims = cosine_similarity(centroid, tfidf_matrix).flatten()

# # # #     recs = []
# # # #     for idx in sims.argsort()[::-1]:
# # # #         tk = stock_keys[idx]
# # # #         if tk in holdings:
# # # #             continue
# # # #         m = stock_meta[tk]
# # # #         recs.append(Recommendation(
# # # #             ticker=tk,
# # # #             name=m["name"],
# # # #             sector=m["sector"],
# # # #             similarity_score=round(float(sims[idx]), 3)
# # # #         ))
# # # #         if len(recs) >= 5:
# # # #             break
# # # #     return recs


# # # # # ─── News Impact Endpoint (no delta, clickable link, RSS summary) ─────────────
# # # # @app.get("/news-impact", response_model=List[NewsImpactItem])
# # # # def news_impact(
# # # #     background_tasks: BackgroundTasks,
# # # #     db: Session            = Depends(get_db),
# # # #     window: int            = Query(60, ge=1),
# # # #     min_sentiment: float   = Query(0.0, ge=0.0),
# # # #     recent_hours: int      = Query(24, ge=1),
# # # #     limit: int             = Query(20, ge=1, le=100),
# # # # ):
# # # #     # trigger RSS crawl in background
# # # #     background_tasks.add_task(crawl.main)

# # # #     try:
# # # #         now    = datetime.datetime.utcnow()
# # # #         cutoff = now - datetime.timedelta(hours=recent_hours)

# # # #         holdings = db.query(HoldingModel).all()
# # # #         recs     = recommend(db)
# # # #         tickers  = {h.ticker for h in holdings} | {r.ticker for r in recs}
# # # #         ticker_rx = re.compile(r"\b(" + "|".join(map(re.escape, tickers)) + r")\b")

# # # #         seen_links = set()
# # # #         scored     = []

# # # #         for fn in glob.glob("data/raw_articles/rss_articles_*.json"):
# # # #             for art in json.loads(Path(fn).read_text()):
# # # #                 link = art.get("link","")
# # # #                 if not link or link in seen_links:
# # # #                     continue
# # # #                 seen_links.add(link)

# # # #                 pub = art.get("published","")
# # # #                 try:
# # # #                     dt = (datetime.datetime.fromisoformat(pub[:-1] + "+00:00")
# # # #                           if pub.endswith("Z")
# # # #                           else datetime.datetime.fromisoformat(pub))
# # # #                     pub_dt = dt.replace(tzinfo=None)
# # # #                 except:
# # # #                     continue
# # # #                 if pub_dt < cutoff:
# # # #                     continue

# # # #                 text = f"{art.get('title','')} {art.get('summary','')}"
# # # #                 m = ticker_rx.search(text)
# # # #                 if not m:
# # # #                     continue
# # # #                 tick = m.group(1).upper()

# # # #                 pol = TextBlob(text).sentiment.polarity
# # # #                 if abs(pol) < min_sentiment:
# # # #                     continue

# # # #                 try:
# # # #                     df = yf.Ticker(tick.replace(".", "-")).history(
# # # #                         start=pub_dt - datetime.timedelta(minutes=30),
# # # #                         end=  pub_dt + datetime.timedelta(minutes=window+30),
# # # #                         interval="1m"
# # # #                     )
# # # #                     if df is None or df.empty:
# # # #                         continue
# # # #                     closes = df["Close"].tolist()
# # # #                 except:
# # # #                     continue

# # # #                 delta = closes[min(window, len(closes)-1)] - closes[0]
# # # #                 impact = round(abs(delta) * abs(pol), 4)

# # # #                 scored.append({
# # # #                     "ticker":    tick,
# # # #                     "headline":  art["title"],
# # # #                     "link":      link,
# # # #                     "source":    art.get("source",""),
# # # #                     "sentiment": round(pol, 3),
# # # #                     "impact":    impact,
# # # #                     "spark":     closes,
# # # #                     "summary":   art.get("summary",""),
# # # #                     "published": pub_dt
# # # #                 })

# # # #         top = sorted(scored, key=lambda x: x["impact"], reverse=True)[:limit]
# # # #         return top

# # # #     except Exception:
# # # #         logging.exception("Error in /news-impact")
# # # #         raise HTTPException(500, "News impact processing failed")

# # # import os
# # # import ssl
# # # import uuid
# # # import datetime
# # # import glob
# # # import json
# # # import logging
# # # import warnings
# # # from pathlib import Path
# # # from typing import List

# # # # suppress bs4 & urllib3 warnings
# # # from bs4 import GuessedAtParserWarning
# # # from requests.packages.urllib3.exceptions import InsecureRequestWarning
# # # warnings.filterwarnings("ignore", category=GuessedAtParserWarning)
# # # warnings.filterwarnings("ignore", category=InsecureRequestWarning)

# # # # silence yfinance noise
# # # logging.getLogger("yfinance").setLevel(logging.CRITICAL)
# # # logging.getLogger("yfinance.ticker").setLevel(logging.CRITICAL)

# # # import yfinance as yf
# # # import pandas as pd
# # # from fastapi import (
# # #     FastAPI, HTTPException, Depends, Query, BackgroundTasks
# # # )
# # # from fastapi.middleware.cors import CORSMiddleware
# # # from fastapi.responses import FileResponse
# # # from pydantic import BaseModel
# # # from sqlalchemy import create_engine, Column, String, Float
# # # from sqlalchemy.ext.declarative import declarative_base
# # # from sqlalchemy.orm import sessionmaker, Session
# # # from sklearn.feature_extraction.text import TfidfVectorizer
# # # from sklearn.metrics.pairwise import cosine_similarity
# # # from textblob import TextBlob
# # # import re

# # # import crawl  # your crawler script

# # # # load env / configure Gemini (even if unused here)
# # # from dotenv import load_dotenv
# # # load_dotenv()
# # # import google.generativeai as genai
# # # genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# # # # SSL
# # # ssl._create_default_https_context = ssl._create_unverified_context

# # # # FastAPI
# # # app = FastAPI(title="StockRadar Core API")
# # # app.add_middleware(
# # #     CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"]
# # # )
# # # @app.get("/", include_in_schema=False)
# # # def serve_ui():
# # #     return FileResponse("index.html")


# # # # Database
# # # DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./stockradar.db")
# # # engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
# # # SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
# # # Base = declarative_base()

# # # class HoldingModel(Base):
# # #     __tablename__ = "portfolio"
# # #     id        = Column(String(36), primary_key=True, index=True)
# # #     ticker    = Column(String(10), nullable=False, unique=True, index=True)
# # #     quantity  = Column(Float, nullable=False)
# # #     avg_price = Column(Float, nullable=False)
# # # Base.metadata.create_all(bind=engine)

# # # def get_db():
# # #     db = SessionLocal()
# # #     try:
# # #         yield db
# # #     finally:
# # #         db.close()


# # # # Schemas
# # # class Holding(BaseModel):
# # #     id: str
# # #     ticker: str
# # #     quantity: float
# # #     avg_price: float

# # # class HoldingCreate(BaseModel):
# # #     ticker: str
# # #     quantity: float

# # # class Recommendation(BaseModel):
# # #     ticker: str
# # #     name: str
# # #     sector: str
# # #     similarity_score: float

# # # class Quote(BaseModel):
# # #     ticker: str
# # #     name: str
# # #     price: float
# # #     change: float
# # #     spark: List[float]

# # # class Performer(BaseModel):
# # #     ticker: str
# # #     name: str
# # #     sector: str
# # #     change_percent: float

# # # class NewsImpactItem(BaseModel):
# # #     ticker: str
# # #     headline: str
# # #     link: str
# # #     source: str
# # #     sentiment: float
# # #     impact: float
# # #     spark: List[float]
# # #     summary: str
# # #     published: datetime.datetime

# # # class StockItem(BaseModel):
# # #     ticker: str
# # #     name: str


# # # # Load S&P 500 metadata
# # # wiki_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
# # # df = pd.read_html(wiki_url, flavor="lxml")[0]
# # # stock_keys = df["Symbol"].tolist()
# # # stock_meta = {
# # #     row.Symbol: {"name": row.Security, "sector": row["GICS Sector"]}
# # #     for _, row in df.iterrows()
# # # }

# # # # TF-IDF setup
# # # tfidf_corpus     = [f"{stock_meta[t]['name']} {stock_meta[t]['sector']}" for t in stock_keys]
# # # tfidf_vectorizer = TfidfVectorizer(stop_words="english")
# # # tfidf_matrix     = tfidf_vectorizer.fit_transform(tfidf_corpus)


# # # # Portfolio CRUD
# # # @app.get("/portfolio", response_model=List[Holding])
# # # def list_portfolio(db: Session = Depends(get_db)):
# # #     return [Holding(**r.__dict__) for r in db.query(HoldingModel).all()]

# # # @app.post("/portfolio", response_model=Holding, status_code=201)
# # # def add_holding(h: HoldingCreate, db: Session = Depends(get_db)):
# # #     t = h.ticker.upper()
# # #     if db.query(HoldingModel).filter_by(ticker=t).first():
# # #         raise HTTPException(400, "Ticker already in portfolio")
# # #     yf_sym = t.replace(".", "-")
# # #     try:
# # #         hist = yf.Ticker(yf_sym).history(period="1d")
# # #         avg_price = float(hist["Close"].iloc[-1])
# # #     except:
# # #         avg_price = 0.0
# # #     rec = HoldingModel(
# # #         id=uuid.uuid4().hex, ticker=t,
# # #         quantity=h.quantity, avg_price=avg_price
# # #     )
# # #     db.add(rec); db.commit(); db.refresh(rec)
# # #     return Holding(**rec.__dict__)

# # # @app.put("/portfolio/{holding_id}", response_model=Holding)
# # # def update_holding(holding_id: str, h: HoldingCreate, db: Session = Depends(get_db)):
# # #     rec = db.query(HoldingModel).get(holding_id)
# # #     if not rec:
# # #         raise HTTPException(404, "Holding not found")
# # #     t = h.ticker.upper()
# # #     yf_sym = t.replace(".", "-")
# # #     try:
# # #         hist = yf.Ticker(yf_sym).history(period="1d")
# # #         avg_price = float(hist["Close"].iloc[-1])
# # #     except:
# # #         avg_price = rec.avg_price
# # #     rec.ticker, rec.quantity, rec.avg_price = t, h.quantity, avg_price
# # #     db.commit(); db.refresh(rec)
# # #     return Holding(**rec.__dict__)

# # # @app.delete("/portfolio/{holding_id}", status_code=204)
# # # def delete_holding(holding_id: str, db: Session = Depends(get_db)):
# # #     db.query(HoldingModel).filter_by(id=holding_id).delete()
# # #     db.commit()


# # # # Stocks list
# # # @app.get("/stocks", response_model=List[StockItem])
# # # def list_stocks():
# # #     return [{"ticker": tk, "name": stock_meta[tk]["name"]} for tk in stock_keys]


# # # # Quotes
# # # @app.get("/quotes", response_model=List[Quote])
# # # def get_quotes(db: Session = Depends(get_db)):
# # #     out = []
# # #     for h in db.query(HoldingModel).all():
# # #         yf_sym = h.ticker.replace(".", "-")
# # #         try:
# # #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# # #             if hist is None or hist.empty: continue
# # #             op, cl = hist["Open"].tolist()[0], hist["Close"].tolist()[-1]
# # #             out.append(Quote(
# # #                 ticker=h.ticker,
# # #                 name=stock_meta.get(h.ticker,{}).get("name",h.ticker),
# # #                 price=round(cl, 2),
# # #                 change=round(cl - op, 2),
# # #                 spark=hist["Close"].tolist()
# # #             ))
# # #         except:
# # #             continue
# # #     if not out:
# # #         raise HTTPException(404, "No quote data")
# # #     return out


# # # # Top performers
# # # @app.get("/top-performers", response_model=List[Performer])
# # # def top_performers():
# # #     perf = []
# # #     for tk in stock_keys:
# # #         yf_sym = tk.replace(".", "-")
# # #         try:
# # #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# # #             if hist is None or hist.empty: continue
# # #             opens, closes = hist["Open"].tolist(), hist["Close"].tolist()
# # #             pct = round((closes[-1] - opens[0]) / opens[0] * 100, 2)
# # #             perf.append({
# # #                 "ticker": tk,
# # #                 "name": stock_meta[tk]["name"],
# # #                 "sector": stock_meta[tk]["sector"],
# # #                 "change_percent": pct
# # #             })
# # #         except:
# # #             continue
# # #     if not perf:
# # #         raise HTTPException(404, "No performance data")
# # #     return sorted(perf, key=lambda x: x["change_percent"], reverse=True)[:5]


# # # # TF-IDF recommendations
# # # @app.get("/recommendations", response_model=List[Recommendation])
# # # def recommend(db: Session = Depends(get_db)):
# # #     holdings = [h.ticker for h in db.query(HoldingModel).all()]
# # #     if not holdings:
# # #         raise HTTPException(400, "Portfolio is empty")
# # #     idxs = [stock_keys.index(t) for t in holdings if t in stock_keys]
# # #     if not idxs:
# # #         raise HTTPException(400, "No metadata for portfolio tickers")

# # #     centroid = tfidf_matrix[idxs].mean(axis=0)
# # #     centroid = centroid.A if hasattr(centroid, "A") else centroid.toarray()
# # #     sims = cosine_similarity(centroid, tfidf_matrix).flatten()

# # #     recs = []
# # #     for idx in sims.argsort()[::-1]:
# # #         tk = stock_keys[idx]
# # #         if tk in holdings: continue
# # #         m = stock_meta[tk]
# # #         recs.append(Recommendation(
# # #             ticker=tk, name=m["name"], sector=m["sector"],
# # #             similarity_score=round(float(sims[idx]), 3)
# # #         ))
# # #         if len(recs) >= 5: break
# # #     return recs


# # # # News-impact
# # # @app.get("/news-impact", response_model=List[NewsImpactItem])
# # # def news_impact(
# # #     background_tasks: BackgroundTasks,
# # #     db: Session            = Depends(get_db),
# # #     window: int            = Query(60, ge=1),
# # #     min_sentiment: float   = Query(0.0, ge=0.0),
# # #     recent_hours: int      = Query(24, ge=1),
# # #     limit: int             = Query(20, ge=1, le=100),
# # # ):
# # #     background_tasks.add_task(crawl.main)

# # #     try:
# # #         now    = datetime.datetime.utcnow()
# # #         cutoff = now - datetime.timedelta(hours=recent_hours)

# # #         holdings = db.query(HoldingModel).all()
# # #         recs     = recommend(db)
# # #         tickers  = {h.ticker for h in holdings} | {r.ticker for r in recs}
# # #         ticker_rx = re.compile(r"\b(" + "|".join(map(re.escape, tickers)) + r")\b")

# # #         seen = set()
# # #         scored = []

# # #         for fn in glob.glob("data/raw_articles/rss_articles_*.json"):
# # #             articles = json.loads(Path(fn).read_text())
# # #             for art in articles:
# # #                 link = art.get("link","")
# # #                 if not link or link in seen:
# # #                     continue
# # #                 seen.add(link)

# # #                 pub = art.get("published","")
# # #                 try:
# # #                     dt = (datetime.datetime.fromisoformat(pub[:-1] + "+00:00")
# # #                           if pub.endswith("Z")
# # #                           else datetime.datetime.fromisoformat(pub))
# # #                     pub_dt = dt.replace(tzinfo=None)
# # #                 except:
# # #                     continue
# # #                 if pub_dt < cutoff:
# # #                     continue

# # #                 text = f"{art.get('title','')} {art.get('summary','')}"
# # #                 m = ticker_rx.search(text)
# # #                 if not m:
# # #                     continue
# # #                 tick = m.group(1).upper()

# # #                 pol = TextBlob(text).sentiment.polarity
# # #                 if abs(pol) < min_sentiment:
# # #                     continue

# # #                 try:
# # #                     df = yf.Ticker(tick.replace(".", "-")).history(
# # #                         start=pub_dt - datetime.timedelta(minutes=30),
# # #                         end=  pub_dt + datetime.timedelta(minutes=window+30),
# # #                         interval="1m"
# # #                     )
# # #                     if df is None or df.empty:
# # #                         continue
# # #                     closes = df["Close"].tolist()
# # #                 except:
# # #                     continue

# # #                 delta = closes[min(window, len(closes)-1)] - closes[0]
# # #                 impact = round(abs(delta) * abs(pol), 4)

# # #                 scored.append({
# # #                     "ticker":    tick,
# # #                     "headline":  art["title"],
# # #                     "link":      link,
# # #                     "source":    art.get("source",""),
# # #                     "sentiment": round(pol, 3),
# # #                     "impact":    impact,
# # #                     "spark":     closes,
# # #                     "summary":   art.get("summary",""),
# # #                     "published": pub_dt
# # #                 })

# # #         top = sorted(scored, key=lambda x: x["impact"], reverse=True)[:limit]
# # #         return top

# # #     except Exception:
# # #         logging.exception("Error in /news-impact")
# # #         raise HTTPException(500, "News impact processing failed")

# # import os
# # import ssl
# # import uuid
# # import datetime
# # import glob
# # import json
# # import logging
# # import warnings
# # from pathlib import Path
# # from typing import List

# # # ─── Suppress warnings ─────────────────────────────────────────────────────────
# # from bs4 import BeautifulSoup, GuessedAtParserWarning
# # from requests.packages.urllib3.exceptions import InsecureRequestWarning
# # warnings.filterwarnings("ignore", category=GuessedAtParserWarning)
# # warnings.filterwarnings("ignore", category=InsecureRequestWarning)

# # # ─── Logging & SSL ─────────────────────────────────────────────────────────────
# # ssl._create_default_https_context = ssl._create_unverified_context
# # logging.getLogger("yfinance").setLevel(logging.CRITICAL)
# # logging.getLogger("yfinance.ticker").setLevel(logging.CRITICAL)

# # # ─── Third-party imports ────────────────────────────────────────────────────────
# # import yfinance as yf
# # import pandas as pd
# # from fastapi import (
# #     FastAPI, HTTPException, Depends,
# #     Query, BackgroundTasks
# # )
# # from fastapi.middleware.cors import CORSMiddleware
# # from fastapi.responses import FileResponse
# # from pydantic import BaseModel
# # from sqlalchemy import create_engine, Column, String, Float
# # from sqlalchemy.ext.declarative import declarative_base
# # from sqlalchemy.orm import sessionmaker, Session
# # from sklearn.feature_extraction.text import TfidfVectorizer
# # from sklearn.metrics.pairwise import cosine_similarity
# # from textblob import TextBlob
# # import re

# # import crawl  # your RSS crawler

# # # ─── Load env & configure Gemini (unused here but kept) ────────────────────────
# # from dotenv import load_dotenv
# # load_dotenv()
# # import google.generativeai as genai
# # genai.configure(api_key=os.getenv("GEMINI_API_KEY"))


# # # ─── FastAPI setup ─────────────────────────────────────────────────────────────
# # app = FastAPI(title="StockRadar Core API")
# # app.add_middleware(
# #     CORSMiddleware,
# #     allow_origins=["*"],
# #     allow_methods=["*"],
# #     allow_headers=["*"],
# # )
# # @app.get("/", include_in_schema=False)
# # def serve_ui():
# #     return FileResponse("index.html")


# # # ─── Database (SQLite + SQLAlchemy) ─────────────────────────────────────────────
# # DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./stockradar.db")
# # engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
# # SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
# # Base = declarative_base()

# # class HoldingModel(Base):
# #     __tablename__ = "portfolio"
# #     id        = Column(String(36), primary_key=True, index=True)
# #     ticker    = Column(String(10), nullable=False, unique=True, index=True)
# #     quantity  = Column(Float, nullable=False)
# #     avg_price = Column(Float, nullable=False)
# # Base.metadata.create_all(bind=engine)

# # def get_db():
# #     db = SessionLocal()
# #     try:
# #         yield db
# #     finally:
# #         db.close()


# # # ─── Pydantic Schemas ──────────────────────────────────────────────────────────
# # class Holding(BaseModel):
# #     id: str
# #     ticker: str
# #     quantity: float
# #     avg_price: float

# # class HoldingCreate(BaseModel):
# #     ticker: str
# #     quantity: float

# # class Recommendation(BaseModel):
# #     ticker: str
# #     name: str
# #     sector: str
# #     similarity_score: float

# # class Quote(BaseModel):
# #     ticker: str
# #     name: str
# #     price: float
# #     change: float
# #     spark: List[float]

# # class Performer(BaseModel):
# #     ticker: str
# #     name: str
# #     sector: str
# #     change_percent: float

# # class NewsImpactItem(BaseModel):
# #     ticker: str
# #     headline: str
# #     link: str
# #     source: str
# #     sentiment: float
# #     impact: float
# #     spark: List[float]
# #     summary: str
# #     published: datetime.datetime

# # class StockItem(BaseModel):
# #     ticker: str
# #     name: str


# # # ─── Load S&P 500 metadata ─────────────────────────────────────────────────────
# # wiki_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
# # df = pd.read_html(wiki_url, flavor="lxml")[0]
# # stock_keys = df["Symbol"].tolist()
# # stock_meta = {
# #     row.Symbol: {"name": row.Security, "sector": row["GICS Sector"]}
# #     for _, row in df.iterrows()
# # }


# # # ─── TF-IDF Setup for IR-based recommendations ────────────────────────────────
# # tfidf_corpus     = [f"{stock_meta[t]['name']} {stock_meta[t]['sector']}" for t in stock_keys]
# # tfidf_vectorizer = TfidfVectorizer(stop_words="english")
# # tfidf_matrix     = tfidf_vectorizer.fit_transform(tfidf_corpus)


# # # ─── Portfolio CRUD ────────────────────────────────────────────────────────────
# # @app.get("/portfolio", response_model=List[Holding])
# # def list_portfolio(db: Session = Depends(get_db)):
# #     return [Holding(**r.__dict__) for r in db.query(HoldingModel).all()]

# # @app.post("/portfolio", response_model=Holding, status_code=201)
# # def add_holding(h: HoldingCreate, db: Session = Depends(get_db)):
# #     t = h.ticker.upper()
# #     if db.query(HoldingModel).filter_by(ticker=t).first():
# #         raise HTTPException(400, "Ticker already in portfolio")
# #     yf_sym = t.replace(".", "-")
# #     try:
# #         hist = yf.Ticker(yf_sym).history(period="1d")
# #         avg_price = float(hist["Close"].iloc[-1])
# #     except:
# #         avg_price = 0.0
# #     rec = HoldingModel(
# #         id=uuid.uuid4().hex, ticker=t,
# #         quantity=h.quantity, avg_price=avg_price
# #     )
# #     db.add(rec); db.commit(); db.refresh(rec)
# #     return Holding(**rec.__dict__)

# # @app.put("/portfolio/{holding_id}", response_model=Holding)
# # def update_holding(holding_id: str, h: HoldingCreate, db: Session = Depends(get_db)):
# #     rec = db.query(HoldingModel).get(holding_id)
# #     if not rec:
# #         raise HTTPException(404, "Holding not found")
# #     t = h.ticker.upper()
# #     yf_sym = t.replace(".", "-")
# #     try:
# #         hist = yf.Ticker(yf_sym).history(period="1d")
# #         avg_price = float(hist["Close"].iloc[-1])
# #     except:
# #         avg_price = rec.avg_price
# #     rec.ticker, rec.quantity, rec.avg_price = t, h.quantity, avg_price
# #     db.commit(); db.refresh(rec)
# #     return Holding(**rec.__dict__)

# # @app.delete("/portfolio/{holding_id}", status_code=204)
# # def delete_holding(holding_id: str, db: Session = Depends(get_db)):
# #     db.query(HoldingModel).filter_by(id=holding_id).delete()
# #     db.commit()


# # # ─── Stocks list for autocomplete ──────────────────────────────────────────────
# # @app.get("/stocks", response_model=List[StockItem])
# # def list_stocks():
# #     return [{"ticker": tk, "name": stock_meta[tk]["name"]} for tk in stock_keys]


# # # ─── Quotes ─────────────────────────────────────────────────────────────────────
# # @app.get("/quotes", response_model=List[Quote])
# # def get_quotes(db: Session = Depends(get_db)):
# #     out = []
# #     for h in db.query(HoldingModel).all():
# #         yf_sym = h.ticker.replace(".", "-")
# #         try:
# #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# #             if hist is None or hist.empty: continue
# #             op, cl = hist["Open"].tolist()[0], hist["Close"].tolist()[-1]
# #             out.append(Quote(
# #                 ticker=h.ticker,
# #                 name=stock_meta.get(h.ticker,{}).get("name",h.ticker),
# #                 price=round(cl, 2),
# #                 change=round(cl - op, 2),
# #                 spark=hist["Close"].tolist()
# #             ))
# #         except:
# #             continue
# #     if not out:
# #         raise HTTPException(404, "No quote data")
# #     return out


# # # ─── Top performers ────────────────────────────────────────────────────────────
# # @app.get("/top-performers", response_model=List[Performer])
# # def top_performers():
# #     perf = []
# #     for tk in stock_keys:
# #         yf_sym = tk.replace(".", "-")
# #         try:
# #             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
# #             if hist is None or hist.empty: continue
# #             o, c = hist["Open"].tolist()[0], hist["Close"].tolist()[-1]
# #             pct = round((c - o) / o * 100, 2)
# #             perf.append({
# #                 "ticker": tk,
# #                 "name": stock_meta[tk]["name"],
# #                 "sector": stock_meta[tk]["sector"],
# #                 "change_percent": pct
# #             })
# #         except:
# #             continue
# #     if not perf:
# #         raise HTTPException(404, "No performance data")
# #     return sorted(perf, key=lambda x: x["change_percent"], reverse=True)[:5]


# # # ─── TF-IDF recommendations ────────────────────────────────────────────────────
# # @app.get("/recommendations", response_model=List[Recommendation])
# # def recommend(db: Session = Depends(get_db)):
# #     holdings = [h.ticker for h in db.query(HoldingModel).all()]
# #     if not holdings:
# #         raise HTTPException(400, "Portfolio is empty")
# #     idxs = [stock_keys.index(t) for t in holdings if t in stock_keys]
# #     if not idxs:
# #         raise HTTPException(400, "No metadata for portfolio tickers")

# #     cent = tfidf_matrix[idxs].mean(axis=0)
# #     cent = cent.A if hasattr(cent, "A") else cent.toarray()
# #     sims = cosine_similarity(cent, tfidf_matrix).flatten()

# #     recs = []
# #     for i in sims.argsort()[::-1]:
# #         tk = stock_keys[i]
# #         if tk in holdings: continue
# #         m = stock_meta[tk]
# #         recs.append(Recommendation(
# #             ticker=tk, name=m["name"], sector=m["sector"],
# #             similarity_score=round(float(sims[i]), 3)
# #         ))
# #         if len(recs) >= 5: break
# #     return recs


# # # ─── News Impact ───────────────────────────────────────────────────────────────
# # @app.get("/news-impact", response_model=List[NewsImpactItem])
# # def news_impact(
# #     background_tasks: BackgroundTasks,
# #     db: Session            = Depends(get_db),
# #     window: int            = Query(60, ge=1),
# #     min_sentiment: float   = Query(0.0, ge=0.0),
# #     recent_hours: int      = Query(24, ge=1),
# #     limit: int             = Query(20, ge=1, le=100),
# # ):
# #     # run your crawler in background
# #     background_tasks.add_task(crawl.main)

# #     try:
# #         now    = datetime.datetime.utcnow()
# #         cutoff = now - datetime.timedelta(hours=recent_hours)

# #         holdings = db.query(HoldingModel).all()
# #         recs     = recommend(db)
# #         tickers  = {h.ticker for h in holdings} | {r.ticker for r in recs}
# #         ticker_rx = re.compile(r"\b(" + "|".join(map(re.escape, tickers)) + r")\b")

# #         seen = set()
# #         scored = []

# #         for fn in glob.glob("data/raw_articles/rss_articles_*.json"):
# #             for art in json.loads(Path(fn).read_text()):
# #                 link = art.get("link","")
# #                 if not link or link in seen:
# #                     continue
# #                 seen.add(link)

# #                 # parse published timestamp
# #                 pub = art.get("published","")
# #                 try:
# #                     dt = (datetime.datetime.fromisoformat(pub[:-1] + "+00:00")
# #                           if pub.endswith("Z")
# #                           else datetime.datetime.fromisoformat(pub))
# #                     pub_dt = dt.replace(tzinfo=None)
# #                 except:
# #                     continue
# #                 if pub_dt < cutoff:
# #                     continue

# #                 text = f"{art.get('title','')} {art.get('summary','')}"
# #                 m = ticker_rx.search(text)
# #                 if not m:
# #                     continue
# #                 tick = m.group(1).upper()

# #                 pol = TextBlob(text).sentiment.polarity
# #                 if abs(pol) < min_sentiment:
# #                     continue

# #                 # fetch price window
# #                 try:
# #                     df = yf.Ticker(tick.replace(".", "-")).history(
# #                         start=pub_dt - datetime.timedelta(minutes=30),
# #                         end=  pub_dt + datetime.timedelta(minutes=window+30),
# #                         interval="1m"
# #                     )
# #                     if df is None or df.empty:
# #                         continue
# #                     closes = df["Close"].tolist()
# #                 except:
# #                     continue

# #                 delta = closes[min(window, len(closes)-1)] - closes[0]
# #                 impact = round(abs(delta) * abs(pol), 4)

# #                 # **clean summary HTML**
# #                 raw = art.get("summary","")
# #                 summary = BeautifulSoup(raw, "lxml").get_text().strip()

# #                 scored.append({
# #                     "ticker":    tick,
# #                     "headline":  art["title"],
# #                     "link":      link,             # front-end will render clickable
# #                     "source":    art.get("source",""),
# #                     "sentiment": round(pol, 3),
# #                     "impact":    impact,
# #                     "spark":     closes,
# #                     "summary":   summary,          # now plain‐text
# #                     "published": pub_dt
# #                 })

# #         top = sorted(scored, key=lambda x: x["impact"], reverse=True)[:limit]
# #         return top

# #     except Exception:
# #         logging.exception("Error in /news-impact")
# #         raise HTTPException(500, "News impact processing failed")

# import os
# import ssl
# import uuid
# import datetime
# import glob
# import json
# import logging
# import warnings
# from pathlib import Path
# from typing import List
# from urllib.parse import urlparse
# import httpx
# import logging
# from bs4 import BeautifulSoup
# from fastapi import HTTPException
# # suppress cert & BS4 warnings
# from bs4 import BeautifulSoup, GuessedAtParserWarning
# from requests.packages.urllib3.exceptions import InsecureRequestWarning
# warnings.filterwarnings("ignore", category=GuessedAtParserWarning)
# warnings.filterwarnings("ignore", category=InsecureRequestWarning)

# # silence yfinance
# logging.getLogger("yfinance").setLevel(logging.CRITICAL)
# logging.getLogger("yfinance.ticker").setLevel(logging.CRITICAL)

# import yfinance as yf
# import pandas as pd
# from fastapi import (
#     FastAPI, HTTPException, Depends, Query, BackgroundTasks
# )
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import FileResponse
# from pydantic import BaseModel
# from sqlalchemy import create_engine, Column, String, Float
# from sqlalchemy.ext.declarative import declarative_base
# from sqlalchemy.orm import sessionmaker, Session
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
# from textblob import TextBlob
# import re
# from fastapi.responses import JSONResponse
# import httpx
# from bs4 import BeautifulSoup

# import crawl  # our improved crawler

# # load .env and (optionally) configure Gemini
# from dotenv import load_dotenv
# load_dotenv()
# import google.generativeai as genai
# genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# # enforce unverified SSL for simplicity
# ssl._create_default_https_context = ssl._create_unverified_context

# # ─── FastAPI setup ─────────────────────────────────────────────────────────────
# app = FastAPI(title="StockRadar Core API")
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_methods=["*"],
#     allow_headers=["*"],
# )
# @app.get("/", include_in_schema=False)
# def serve_ui():
#     return FileResponse("index.html")


# # ─── Database (SQLite + SQLAlchemy) ─────────────────────────────────────────────
# DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./stockradar.db")
# engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
# SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
# Base = declarative_base()

# class HoldingModel(Base):
#     __tablename__ = "portfolio"
#     id        = Column(String(36), primary_key=True, index=True)
#     ticker    = Column(String(10), nullable=False, unique=True, index=True)
#     quantity  = Column(Float, nullable=False)
#     avg_price = Column(Float, nullable=False)
# Base.metadata.create_all(bind=engine)

# def get_db():
#     db = SessionLocal()
#     try:
#         yield db
#     finally:
#         db.close()


# # ─── Pydantic Schemas ──────────────────────────────────────────────────────────
# class Holding(BaseModel):
#     id: str
#     ticker: str
#     quantity: float
#     avg_price: float

# class HoldingCreate(BaseModel):
#     ticker: str
#     quantity: float

# class Recommendation(BaseModel):
#     ticker: str
#     name: str
#     sector: str
#     similarity_score: float

# class Quote(BaseModel):
#     ticker: str
#     name: str
#     price: float
#     change: float
#     spark: List[float]

# class Performer(BaseModel):
#     ticker: str
#     name: str
#     sector: str
#     change_percent: float

# class NewsImpactItem(BaseModel):
#     ticker: str
#     headline: str
#     link: str
#     source: str
#     sentiment: float
#     impact: float
#     spark: List[float]
#     summary: str
#     published: datetime.datetime

# class StockItem(BaseModel):
#     ticker: str
#     name: str


# # ─── Load S&P 500 metadata ─────────────────────────────────────────────────────
# wiki_url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
# df = pd.read_html(wiki_url, flavor="lxml")[0]
# stock_keys = df["Symbol"].tolist()
# stock_meta = {
#     row.Symbol: {"name": row.Security, "sector": row["GICS Sector"]}
#     for _, row in df.iterrows()
# }


# # ─── TF-IDF Setup ───────────────────────────────────────────────────────────────
# tfidf_corpus     = [f"{stock_meta[t]['name']} {stock_meta[t]['sector']}" for t in stock_keys]
# tfidf_vectorizer = TfidfVectorizer(stop_words="english")
# tfidf_matrix     = tfidf_vectorizer.fit_transform(tfidf_corpus)


# # ─── Portfolio CRUD ────────────────────────────────────────────────────────────
# @app.get("/portfolio", response_model=List[Holding])
# def list_portfolio(db: Session = Depends(get_db)):
#     return [Holding(**r.__dict__) for r in db.query(HoldingModel).all()]

# @app.post("/portfolio", response_model=Holding, status_code=201)
# def add_holding(h: HoldingCreate, db: Session = Depends(get_db)):
#     t = h.ticker.upper()
#     if db.query(HoldingModel).filter_by(ticker=t).first():
#         raise HTTPException(400, "Ticker already in portfolio")
#     yf_sym = t.replace(".", "-")
#     try:
#         hist = yf.Ticker(yf_sym).history(period="1d")
#         avg_price = float(hist["Close"].iloc[-1])
#     except:
#         avg_price = 0.0
#     rec = HoldingModel(
#         id=uuid.uuid4().hex,
#         ticker=t,
#         quantity=h.quantity,
#         avg_price=avg_price
#     )
#     db.add(rec); db.commit(); db.refresh(rec)
#     return Holding(**rec.__dict__)

# @app.put("/portfolio/{holding_id}", response_model=Holding)
# def update_holding(holding_id: str, h: HoldingCreate, db: Session = Depends(get_db)):
#     rec = db.query(HoldingModel).get(holding_id)
#     if not rec:
#         raise HTTPException(404, "Holding not found")
#     t = h.ticker.upper()
#     yf_sym = t.replace(".", "-")
#     try:
#         hist = yf.Ticker(yf_sym).history(period="1d")
#         avg_price = float(hist["Close"].iloc[-1])
#     except:
#         avg_price = rec.avg_price
#     rec.ticker, rec.quantity, rec.avg_price = t, h.quantity, avg_price
#     db.commit(); db.refresh(rec)
#     return Holding(**rec.__dict__)

# @app.delete("/portfolio/{holding_id}", status_code=204)
# def delete_holding(holding_id: str, db: Session = Depends(get_db)):
#     db.query(HoldingModel).filter_by(id=holding_id).delete()
#     db.commit()


# # ─── Stocks list ───────────────────────────────────────────────────────────────
# @app.get("/stocks", response_model=List[StockItem])
# def list_stocks():
#     return [{"ticker": tk, "name": stock_meta[tk]["name"]} for tk in stock_keys]


# # ─── Quotes ─────────────────────────────────────────────────────────────────────
# @app.get("/quotes", response_model=List[Quote])
# def get_quotes(db: Session = Depends(get_db)):
#     out = []
#     for h in db.query(HoldingModel).all():
#         yf_sym = h.ticker.replace(".", "-")
#         try:
#             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
#             if hist is None or hist.empty:
#                 continue
#             op, cl = hist["Open"].tolist()[0], hist["Close"].tolist()[-1]
#             out.append(Quote(
#                 ticker=h.ticker,
#                 name=stock_meta.get(h.ticker,{}).get("name",h.ticker),
#                 price=round(cl, 2),
#                 change=round(cl - op, 2),
#                 spark=hist["Close"].tolist()
#             ))
#         except:
#             continue
#     if not out:
#         raise HTTPException(404, "No quote data")
#     return out


# # ─── Top performers ────────────────────────────────────────────────────────────
# @app.get("/top-performers", response_model=List[Performer])
# def top_performers():
#     perf = []
#     for tk in stock_keys:
#         yf_sym = tk.replace(".", "-")
#         try:
#             hist = yf.Ticker(yf_sym).history(period="1d", interval="1m")
#             if hist is None or hist.empty:
#                 continue
#             o, c = hist["Open"].tolist()[0], hist["Close"].tolist()[-1]
#             pct = round((c - o) / o * 100, 2)
#             perf.append({
#                 "ticker": tk,
#                 "name": stock_meta[tk]["name"],
#                 "sector": stock_meta[tk]["sector"],
#                 "change_percent": pct
#             })
#         except:
#             continue
#     if not perf:
#         raise HTTPException(404, "No performance data")
#     return sorted(perf, key=lambda x: x["change_percent"], reverse=True)[:5]


# # ─── TF-IDF recommendations ────────────────────────────────────────────────────
# @app.get("/recommendations", response_model=List[Recommendation])
# def recommend(db: Session = Depends(get_db)):
#     holdings = [h.ticker for h in db.query(HoldingModel).all()]
#     if not holdings:
#         raise HTTPException(400, "Portfolio is empty")
#     idxs = [stock_keys.index(t) for t in holdings if t in stock_keys]
#     if not idxs:
#         raise HTTPException(400, "No metadata for portfolio tickers")

#     centroid = tfidf_matrix[idxs].mean(axis=0)
#     centroid = centroid.A if hasattr(centroid, "A") else centroid.toarray()
#     sims = cosine_similarity(centroid, tfidf_matrix).flatten()

#     recs = []
#     for i in sims.argsort()[::-1]:
#         tk = stock_keys[i]
#         if tk in holdings: continue
#         m = stock_meta[tk]
#         recs.append(Recommendation(
#             ticker=tk, name=m["name"], sector=m["sector"],
#             similarity_score=round(float(sims[i]), 3)
#         ))
#         if len(recs) >= 5: break
#     return recs


# # ─── News Impact ───────────────────────────────────────────────────────────────
# @app.get("/news-impact", response_model=List[NewsImpactItem])
# def news_impact(
#     background_tasks: BackgroundTasks,
#     db: Session            = Depends(get_db),
#     window: int            = Query(60, ge=1),
#     min_sentiment: float   = Query(0.0, ge=0.0),
#     recent_hours: int      = Query(24, ge=1),
#     limit: int             = Query(20, ge=1, le=100),
# ):
#     background_tasks.add_task(crawl.main)

#     try:
#         now    = datetime.datetime.utcnow()
#         cutoff = now - datetime.timedelta(hours=recent_hours)

#         holdings = db.query(HoldingModel).all()
#         recs     = recommend(db)
#         tickers  = {h.ticker for h in holdings} | {r.ticker for r in recs}
#         ticker_rx = re.compile(r"\b(" + "|".join(map(re.escape, tickers)) + r")\b")

#         seen = set()
#         scored = []

#         for fn in glob.glob("data/raw_articles/rss_articles_*.json"):
#             for art in json.loads(Path(fn).read_text()):
#                 link = art.get("link","")
#                 if not link or link in seen:
#                     continue
#                 seen.add(link)

#                 # parse published timestamp
#                 pub = art.get("published","")
#                 try:
#                     dt = (datetime.datetime.fromisoformat(pub[:-1] + "+00:00")
#                           if pub.endswith("Z")
#                           else datetime.datetime.fromisoformat(pub))
#                     pub_dt = dt.replace(tzinfo=None)
#                 except:
#                     continue
#                 if pub_dt < cutoff:
#                     continue

#                 text = f"{art.get('title','')} {art.get('summary','')}"
#                 m = ticker_rx.search(text)
#                 if not m:
#                     continue
#                 tick = m.group(1).upper()

#                 pol = TextBlob(text).sentiment.polarity
#                 if abs(pol) < min_sentiment:
#                     continue

#                 try:
#                     df = yf.Ticker(tick.replace(".", "-")).history(
#                         start=pub_dt - datetime.timedelta(minutes=30),
#                         end=  pub_dt + datetime.timedelta(minutes=window+30),
#                         interval="1m"
#                     )
#                     if df is None or df.empty:
#                         continue
#                     closes = df["Close"].tolist()
#                 except:
#                     continue

#                 delta = closes[min(window, len(closes)-1)] - closes[0]
#                 impact = round(abs(delta) * abs(pol), 4)

#                 # clean summary (drop HTML and avoid title==summary)
#                 raw     = art.get("summary","") or ""
#                 clean   = BeautifulSoup(raw, "lxml").get_text().strip()
#                 title   = art.get("title","").strip()
#                 if clean == title:
#                     clean = ""

#                 scored.append({
#                     "ticker":    tick,
#                     "headline":  title,
#                     "link":      link,
#                     "source":    art.get("source",""),
#                     "sentiment": round(pol, 3),
#                     "impact":    impact,
#                     "spark":     closes,
#                     "summary":   clean,
#                     "published": pub_dt
#                 })

#         top = sorted(scored, key=lambda x: x["impact"], reverse=True)[:limit]
#         return top

#     except Exception:
#         logging.exception("Error in /news-impact")
#         raise HTTPException(500, "News impact processing failed")
    
    
# # @app.get("/article")
# # async def fetch_article(url: str):
# #     """
# #     Fetch the given URL, follow redirects, detect Google News wrappers,
# #     then fetch the real publisher page, and return its paragraph text.
# #     """
# #     try:
# #         async with httpx.AsyncClient(
# #             timeout=10.0,
# #             verify=False,
# #             follow_redirects=True,
# #             headers={"User-Agent": "Mozilla/5.0"}
# #         ) as client:
# #             resp = await client.get(url)
# #             resp.raise_for_status()

# #         soup = BeautifulSoup(resp.text, "lxml")

# #         # If we're on a Google News wrapper, find the real URL in og:url
# #         domain = urlparse(resp.url).netloc
# #         if "news.google.com" in domain:
# #             og = soup.find("meta", property="og:url")
# #             if og and og.get("content"):
# #                 real_url = og["content"]
# #                 # fetch the real article
# #                 async with httpx.AsyncClient(
# #                     timeout=10.0,
# #                     verify=False,
# #                     headers={"User-Agent": "Mozilla/5.0"}
# #                 ) as client2:
# #                     resp2 = await client2.get(real_url)
# #                     resp2.raise_for_status()
# #                 soup = BeautifulSoup(resp2.text, "lxml")

# #         # Try to grab <article> or <main>, else whole page
# #         container = soup.find("article") or soup.find("main") or soup

# #         paras = container.find_all("p")
# #         if paras:
# #             body = "\n\n".join(p.get_text(strip=True) for p in paras)
# #         else:
# #             # fallback to all text
# #             body = soup.get_text(separator="\n\n", strip=True)

# #         if not body:
# #             body = "No textual content could be extracted."

# #         return {"body": body}

# #     except Exception as e:
# #         logging.exception("Article fetch failed for %s", url)
# #         raise HTTPException(500, f"Failed to fetch article: {e}")

# @app.get("/article")
# @app.get("/article")
# async def fetch_article(url: str):
#     """
#     1) Follow the RSS link (which may be a Google News wrapper).
#     2) Parse out <link rel="canonical"> or og:url to get the true publisher URL.
#     3) Fetch that real URL and extract <p> text.
#     4) Fall back to the RSS summary if all else fails.
#     """
#     try:
#         async with httpx.AsyncClient(
#             timeout=10.0,
#             verify=False,
#             follow_redirects=True,
#             headers={"User-Agent": "Mozilla/5.0"}
#         ) as client:
#             resp = await client.get(url)
#             resp.raise_for_status()

#         soup = BeautifulSoup(resp.text, "lxml")

#         # 1️⃣ Try <link rel="canonical">
#         can = soup.find("link", rel="canonical")
#         real_url = can["href"] if (can and can.get("href")) else None

#         # 2️⃣ If no canonical, try Open Graph URL
#         if not real_url:
#             og = soup.find("meta", property="og:url")
#             real_url = og["content"] if (og and og.get("content")) else None

#         # 3️⃣ If we got a real URL and it's different, fetch that
#         if real_url and real_url != str(resp.url):
#             async with httpx.AsyncClient(
#                 timeout=10.0,
#                 verify=False,
#                 follow_redirects=True,
#                 headers={"User-Agent": "Mozilla/5.0"}
#             ) as client2:
#                 resp2 = await client2.get(real_url)
#                 resp2.raise_for_status()
#             soup = BeautifulSoup(resp2.text, "lxml")

#         # 4️⃣ Try extracting <p> tags from <article> or <main>
#         container = soup.find("article") or soup.find("main") or soup
#         paras = container.find_all("p")
#         if paras:
#             body = "\n\n".join(p.get_text(strip=True) for p in paras)
#         else:
#             # As a last resort, grab all text, but this may include nav/footer
#             body = soup.get_text(separator="\n\n", strip=True)

#         if not body:
#             body = "No textual content could be extracted."

#         return {"body": body}

#     except Exception as e:
#         logging.exception("Article fetch failed for %s", url)
#         raise HTTPException(500, f"Failed to fetch article: {e}")
